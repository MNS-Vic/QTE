"""
QTEæ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤º
å±•ç¤ºå®Œæ•´çš„æ•°æ®æºåŠŸèƒ½ï¼šå¤šæ•°æ®æºåˆ‡æ¢ã€æ•°æ®è´¨é‡æ£€æŸ¥ã€æ€§èƒ½å¯¹æ¯”ã€æ•°æ®èšåˆ
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import json
import time
import threading
from typing import Dict, List, Optional, Any
import warnings
warnings.filterwarnings('ignore')

try:
    from qte.data.sources.local_csv import LocalCsvSource
    from qte.data.sources.gm_quant import GmQuantSource
    from qte.data.sources.binance_api import BinanceApiSource
    DATASOURCE_AVAILABLE = True
except ImportError as e:
    print(f"Warning: QTE DataSource modules import failed: {e}")
    DATASOURCE_AVAILABLE = False
    
    # æä¾›Mockç±»
    class MockDataSource:
        def __init__(self, *args, **kwargs):
            self.name = kwargs.get('name', 'MockSource')
            self.connected = False
            
        def connect(self):
            self.connected = True
            return True
            
        def get_bars(self, symbol, start_date=None, end_date=None):
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            dates = pd.date_range(
                start=start_date or (datetime.now() - timedelta(days=30)),
                end=end_date or datetime.now(),
                freq='D'
            )
            
            data = []
            base_price = np.random.uniform(50, 500)
            
            for date in dates:
                price = base_price * (1 + np.random.normal(0, 0.02))
                data.append({
                    'date': date,
                    'open': price * np.random.uniform(0.99, 1.01),
                    'high': price * np.random.uniform(1.01, 1.03),
                    'low': price * np.random.uniform(0.97, 0.99),
                    'close': price,
                    'volume': np.random.randint(100000, 1000000)
                })
            
            return pd.DataFrame(data)
        
        def disconnect(self):
            self.connected = False
    
    LocalCsvSource = MockDataSource
    GmQuantSource = MockDataSource
    BinanceApiSource = MockDataSource


class DataSourceEcosystemDemo:
    """æ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤ºç±»"""
    
    def __init__(self):
        self.logger = logging.getLogger('DataSourceEcosystemDemo')
        self.output_dir = Path('demo_output')
        self.data_dir = Path('demo_data')
        self.output_dir.mkdir(exist_ok=True)
        self.data_dir.mkdir(exist_ok=True)
        
        # æ•°æ®æºæ³¨å†Œè¡¨
        self.data_sources = {}
        self.source_configs = {}
        
        # æ¼”ç¤ºå‚æ•°
        self.symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA']
        self.test_period_days = 30
        
        # ç»“æœå­˜å‚¨
        self.performance_metrics = {}
        self.quality_reports = {}
        self.aggregated_data = {}
        self.benchmark_results = {}
        
    def check_datasource_availability(self):
        """æ£€æŸ¥æ•°æ®æºæ¨¡å—å¯ç”¨æ€§"""
        if not DATASOURCE_AVAILABLE:
            self.logger.warning("âš ï¸ QTEæ•°æ®æºæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¼”ç¤ºæ¨¡å¼")
            self.logger.info("ğŸ’¡ æ¼”ç¤ºæ¨¡å¼å°†å±•ç¤ºQTEæ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¶æ„å’ŒåŠŸèƒ½")
            return "demo_mode"
        
        self.logger.info("âœ… QTEæ•°æ®æºæ¨¡å—å¯ç”¨")
        return "full_mode"
    
    def setup_data_sources(self):
        """è®¾ç½®å’Œæ³¨å†Œå¤šä¸ªæ•°æ®æº"""
        self.logger.info("ğŸ”§ è®¾ç½®æ•°æ®æºç”Ÿæ€ç³»ç»Ÿ...")
        
        try:
            # 1. æœ¬åœ°CSVæ•°æ®æº
            self.data_sources['local_csv'] = LocalCsvSource(
                name='LocalCSV',
                base_path=str(self.data_dir)
            )
            self.source_configs['local_csv'] = {
                'type': 'Local File',
                'description': 'æœ¬åœ°CSVæ–‡ä»¶æ•°æ®æº',
                'latency': 'Ultra Low',
                'reliability': 'High',
                'cost': 'Free'
            }
            
            # 2. æ˜é‡‘æ•°æ®æºï¼ˆæ¨¡æ‹Ÿï¼‰
            self.data_sources['gm_quant'] = GmQuantSource(
                name='GmQuant',
                token="demo_token_12345"
            )
            self.source_configs['gm_quant'] = {
                'type': 'API Service',
                'description': 'æ˜é‡‘é‡åŒ–æ•°æ®API',
                'latency': 'Low',
                'reliability': 'High',
                'cost': 'Paid'
            }
            
            # 3. å¸å®‰APIæ•°æ®æºï¼ˆæ¨¡æ‹Ÿï¼‰
            self.data_sources['binance_api'] = BinanceApiSource(
                name='BinanceAPI',
                api_key="demo_api_key",
                api_secret="demo_secret"
            )
            self.source_configs['binance_api'] = {
                'type': 'Exchange API',
                'description': 'å¸å®‰äº¤æ˜“æ‰€API',
                'latency': 'Medium',
                'reliability': 'Medium',
                'cost': 'Free'
            }
            
            self.logger.info("âœ… æ•°æ®æºç”Ÿæ€ç³»ç»Ÿè®¾ç½®å®Œæˆ")
            self.logger.info(f"   æ³¨å†Œæ•°æ®æº: {len(self.data_sources)} ä¸ª")
            
            for name, config in self.source_configs.items():
                self.logger.info(f"   - {name}: {config['description']}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æºè®¾ç½®å¤±è´¥: {e}")
            return False
    
    def generate_sample_data_files(self):
        """ç”Ÿæˆç¤ºä¾‹æ•°æ®æ–‡ä»¶"""
        self.logger.info("ğŸ“Š ç”Ÿæˆç¤ºä¾‹æ•°æ®æ–‡ä»¶...")
        
        # ç”Ÿæˆæµ‹è¯•æœŸé—´çš„æ•°æ®
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.test_period_days)
        
        for symbol in self.symbols:
            # è®¾ç½®ä¸åŒçš„åŸºç¡€ä»·æ ¼å’Œæ³¢åŠ¨ç‡
            base_prices = {
                'AAPL': 150.0,
                'GOOGL': 2500.0,
                'MSFT': 300.0,
                'TSLA': 800.0,
                'NVDA': 400.0
            }
            
            volatilities = {
                'AAPL': 0.02,
                'GOOGL': 0.025,
                'MSFT': 0.018,
                'TSLA': 0.04,
                'NVDA': 0.035
            }
            
            base_price = base_prices.get(symbol, 100.0)
            volatility = volatilities.get(symbol, 0.02)
            
            # ç”Ÿæˆæ—¶é—´åºåˆ—
            dates = pd.date_range(start_date, end_date, freq='D')
            
            # ç”Ÿæˆä»·æ ¼æ•°æ®ï¼ˆå‡ ä½•å¸ƒæœ—è¿åŠ¨ï¼‰
            np.random.seed(hash(symbol) % 2**32)
            returns = np.random.normal(0.001, volatility, len(dates))
            
            prices = [base_price]
            for ret in returns[1:]:
                new_price = prices[-1] * (1 + ret)
                prices.append(max(new_price, 1.0))
            
            # æ„é€ OHLCVæ•°æ®
            data = []
            for i, (date, close) in enumerate(zip(dates, prices)):
                high = close * np.random.uniform(1.001, 1.02)
                low = close * np.random.uniform(0.98, 0.999)
                open_price = close * np.random.uniform(0.995, 1.005)
                volume = np.random.uniform(1000000, 10000000)
                
                data.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'open': round(open_price, 2),
                    'high': round(high, 2),
                    'low': round(low, 2),
                    'close': round(close, 2),
                    'volume': int(volume)
                })
            
            # ä¿å­˜CSVæ–‡ä»¶
            df = pd.DataFrame(data)
            csv_file = self.data_dir / f'{symbol}.csv'
            df.to_csv(csv_file, index=False)
            
            self.logger.debug(f"  ç”Ÿæˆ {symbol}.csv: {len(df)} è¡Œæ•°æ®")
        
        self.logger.info(f"âœ… ç¤ºä¾‹æ•°æ®æ–‡ä»¶å·²ç”Ÿæˆ: {self.data_dir}")
        return True
    
    def test_data_source_performance(self):
        """æµ‹è¯•å„æ•°æ®æºçš„æ€§èƒ½"""
        self.logger.info("âš¡ æ‰§è¡Œæ•°æ®æºæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        performance_results = {}
        
        for source_name, source in self.data_sources.items():
            self.logger.info(f"  æµ‹è¯• {source_name}...")
            
            source_metrics = {
                'connection_time': 0,
                'avg_fetch_time': 0,
                'total_data_points': 0,
                'success_rate': 0,
                'error_count': 0,
                'throughput': 0,  # æ•°æ®ç‚¹/ç§’
                'reliability_score': 0
            }
            
            try:
                # æµ‹è¯•è¿æ¥æ—¶é—´
                start_time = time.time()
                connected = source.connect()
                connection_time = time.time() - start_time
                source_metrics['connection_time'] = connection_time
                
                if connected:
                    # æµ‹è¯•æ•°æ®è·å–æ€§èƒ½
                    successful_fetches = 0
                    total_fetch_time = 0
                    total_data_points = 0
                    
                    for symbol in self.symbols:
                        try:
                            fetch_start = time.time()
                            
                            # è·å–æ•°æ®
                            data = source.get_bars(
                                symbol,
                                start_date=datetime.now() - timedelta(days=self.test_period_days),
                                end_date=datetime.now()
                            )
                            
                            fetch_time = time.time() - fetch_start
                            
                            if data is not None and len(data) > 0:
                                successful_fetches += 1
                                total_fetch_time += fetch_time
                                total_data_points += len(data)
                                
                                self.logger.debug(f"    {symbol}: {len(data)} æ¡æ•°æ®ï¼Œè€—æ—¶ {fetch_time:.3f}s")
                            
                        except Exception as e:
                            source_metrics['error_count'] += 1
                            self.logger.debug(f"    {symbol} è·å–å¤±è´¥: {e}")
                    
                    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                    if successful_fetches > 0:
                        source_metrics['avg_fetch_time'] = total_fetch_time / successful_fetches
                        source_metrics['total_data_points'] = total_data_points
                        source_metrics['success_rate'] = successful_fetches / len(self.symbols)
                        source_metrics['throughput'] = total_data_points / total_fetch_time if total_fetch_time > 0 else 0
                        
                        # è®¡ç®—å¯é æ€§è¯„åˆ† (0-100)
                        reliability_score = (
                            source_metrics['success_rate'] * 40 +  # æˆåŠŸç‡æƒé‡40%
                            min(1.0, 1.0 / max(source_metrics['avg_fetch_time'], 0.001)) * 30 +  # é€Ÿåº¦æƒé‡30%
                            (1.0 - source_metrics['error_count'] / len(self.symbols)) * 30  # é”™è¯¯ç‡æƒé‡30%
                        )
                        source_metrics['reliability_score'] = min(100, max(0, reliability_score))
                
                else:
                    source_metrics['error_count'] = len(self.symbols)
                
                # æ–­å¼€è¿æ¥
                if hasattr(source, 'disconnect'):
                    source.disconnect()
                
            except Exception as e:
                source_metrics['error_count'] = len(self.symbols)
                self.logger.debug(f"  {source_name} æµ‹è¯•å¤±è´¥: {e}")
            
            performance_results[source_name] = source_metrics
            
            # æ‰“å°æ€§èƒ½æ‘˜è¦
            self.logger.info(f"    è¿æ¥æ—¶é—´: {source_metrics['connection_time']:.3f}s")
            self.logger.info(f"    å¹³å‡è·å–æ—¶é—´: {source_metrics['avg_fetch_time']:.3f}s")
            self.logger.info(f"    æˆåŠŸç‡: {source_metrics['success_rate']:.1%}")
            self.logger.info(f"    ååé‡: {source_metrics['throughput']:.1f} æ•°æ®ç‚¹/ç§’")
            self.logger.info(f"    å¯é æ€§è¯„åˆ†: {source_metrics['reliability_score']:.1f}/100")
        
        self.performance_metrics = performance_results
        self.logger.info("âœ… æ•°æ®æºæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        return performance_results

    def perform_data_quality_analysis(self):
        """æ‰§è¡Œæ•°æ®è´¨é‡åˆ†æ"""
        self.logger.info("ğŸ” æ‰§è¡Œæ•°æ®è´¨é‡åˆ†æ...")

        quality_reports = {}

        for source_name, source in self.data_sources.items():
            self.logger.info(f"  åˆ†æ {source_name} æ•°æ®è´¨é‡...")

            source_quality = {
                'total_symbols': 0,
                'valid_symbols': 0,
                'missing_data_ratio': 0,
                'duplicate_ratio': 0,
                'outlier_ratio': 0,
                'data_completeness': 0,
                'quality_score': 0,
                'issues': []
            }

            try:
                source.connect()

                total_records = 0
                missing_records = 0
                duplicate_records = 0
                outlier_records = 0

                for symbol in self.symbols:
                    try:
                        data = source.get_bars(
                            symbol,
                            start_date=datetime.now() - timedelta(days=self.test_period_days),
                            end_date=datetime.now()
                        )

                        source_quality['total_symbols'] += 1

                        if data is not None and len(data) > 0:
                            source_quality['valid_symbols'] += 1
                            total_records += len(data)

                            # æ£€æŸ¥ç¼ºå¤±å€¼
                            missing_count = data.isnull().sum().sum()
                            missing_records += missing_count

                            # æ£€æŸ¥é‡å¤å€¼
                            if 'date' in data.columns:
                                duplicate_count = data.duplicated(subset=['date']).sum()
                                duplicate_records += duplicate_count

                            # æ£€æŸ¥ä»·æ ¼å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨3Ïƒè§„åˆ™ï¼‰
                            if 'close' in data.columns:
                                price_mean = data['close'].mean()
                                price_std = data['close'].std()
                                outliers = ((data['close'] - price_mean).abs() > 3 * price_std).sum()
                                outlier_records += outliers

                            # è®°å½•å…·ä½“é—®é¢˜
                            if missing_count > 0:
                                source_quality['issues'].append(f"{symbol}: {missing_count}ä¸ªç¼ºå¤±å€¼")
                            if duplicate_count > 0:
                                source_quality['issues'].append(f"{symbol}: {duplicate_count}ä¸ªé‡å¤å€¼")
                            if outliers > 0:
                                source_quality['issues'].append(f"{symbol}: {outliers}ä¸ªå¼‚å¸¸å€¼")

                        else:
                            source_quality['issues'].append(f"{symbol}: æ— æ³•è·å–æ•°æ®")

                    except Exception as e:
                        source_quality['issues'].append(f"{symbol}: æ•°æ®è·å–å¤±è´¥ - {str(e)}")

                # è®¡ç®—è´¨é‡æŒ‡æ ‡
                if source_quality['total_symbols'] > 0:
                    source_quality['data_completeness'] = source_quality['valid_symbols'] / source_quality['total_symbols']

                if total_records > 0:
                    source_quality['missing_data_ratio'] = missing_records / total_records
                    source_quality['duplicate_ratio'] = duplicate_records / total_records
                    source_quality['outlier_ratio'] = outlier_records / total_records

                    # è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ† (0-100)
                    quality_score = (
                        source_quality['data_completeness'] * 40 +  # æ•°æ®å®Œæ•´æ€§æƒé‡40%
                        (1 - source_quality['missing_data_ratio']) * 25 +  # ç¼ºå¤±å€¼æƒé‡25%
                        (1 - source_quality['duplicate_ratio']) * 20 +  # é‡å¤å€¼æƒé‡20%
                        (1 - source_quality['outlier_ratio']) * 15  # å¼‚å¸¸å€¼æƒé‡15%
                    ) * 100

                    source_quality['quality_score'] = min(100, max(0, quality_score))

                if hasattr(source, 'disconnect'):
                    source.disconnect()

            except Exception as e:
                source_quality['issues'].append(f"æ•°æ®æºåˆ†æå¤±è´¥: {str(e)}")

            quality_reports[source_name] = source_quality

            # æ‰“å°è´¨é‡æ‘˜è¦
            self.logger.info(f"    æœ‰æ•ˆæ ‡çš„: {source_quality['valid_symbols']}/{source_quality['total_symbols']}")
            self.logger.info(f"    æ•°æ®å®Œæ•´æ€§: {source_quality['data_completeness']:.1%}")
            self.logger.info(f"    è´¨é‡è¯„åˆ†: {source_quality['quality_score']:.1f}/100")
            if source_quality['issues']:
                self.logger.info(f"    å‘ç°é—®é¢˜: {len(source_quality['issues'])}ä¸ª")

        self.quality_reports = quality_reports
        self.logger.info("âœ… æ•°æ®è´¨é‡åˆ†æå®Œæˆ")
        return quality_reports

    def demonstrate_data_aggregation(self):
        """æ¼”ç¤ºæ•°æ®èšåˆå’ŒåŒæ­¥åŠŸèƒ½"""
        self.logger.info("ğŸ”„ æ¼”ç¤ºæ•°æ®èšåˆå’ŒåŒæ­¥åŠŸèƒ½...")

        aggregation_results = {}

        try:
            # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•æ ‡çš„
            test_symbol = 'AAPL'
            source_data = {}

            # ä»æ‰€æœ‰æ•°æ®æºè·å–åŒä¸€æ ‡çš„çš„æ•°æ®
            for source_name, source in self.data_sources.items():
                try:
                    source.connect()
                    data = source.get_bars(
                        test_symbol,
                        start_date=datetime.now() - timedelta(days=self.test_period_days),
                        end_date=datetime.now()
                    )

                    if data is not None and len(data) > 0:
                        source_data[source_name] = data
                        self.logger.info(f"  {source_name}: è·å– {len(data)} æ¡æ•°æ®")

                    if hasattr(source, 'disconnect'):
                        source.disconnect()

                except Exception as e:
                    self.logger.debug(f"  {source_name}: æ•°æ®è·å–å¤±è´¥ - {e}")

            if len(source_data) > 1:
                # æ•°æ®èšåˆç­–ç•¥æ¼”ç¤º
                self.logger.info("  æ‰§è¡Œæ•°æ®èšåˆç­–ç•¥...")

                # ç­–ç•¥1: æ•°æ®æºä¼˜å…ˆçº§æ’åº
                source_priority = {
                    'local_csv': 3,      # æœ€é«˜ä¼˜å…ˆçº§ï¼ˆæœ¬åœ°æ•°æ®æœ€å¯é ï¼‰
                    'gm_quant': 2,       # ä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆä»˜è´¹APIï¼‰
                    'binance_api': 1     # æœ€ä½ä¼˜å…ˆçº§ï¼ˆå…è´¹APIï¼‰
                }

                # ç­–ç•¥2: æ•°æ®å®Œæ•´æ€§è¯„ä¼°
                completeness_scores = {}
                for source_name, data in source_data.items():
                    missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
                    completeness_scores[source_name] = 1 - missing_ratio

                # ç­–ç•¥3: æ—¶é—´èŒƒå›´è¦†ç›–è¯„ä¼°
                coverage_scores = {}
                for source_name, data in source_data.items():
                    if 'date' in data.columns:
                        date_range = pd.to_datetime(data['date']).max() - pd.to_datetime(data['date']).min()
                        coverage_scores[source_name] = date_range.days
                    else:
                        coverage_scores[source_name] = 0

                # ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³æ•°æ®æº
                best_source = max(source_data.keys(), key=lambda x: (
                    source_priority.get(x, 0) * 0.4 +
                    completeness_scores.get(x, 0) * 0.3 +
                    (coverage_scores.get(x, 0) / max(coverage_scores.values()) if coverage_scores.values() else 0) * 0.3
                ))

                # æ•°æ®äº¤å‰éªŒè¯
                common_dates = None
                for source_name, data in source_data.items():
                    if 'date' in data.columns:
                        dates = set(pd.to_datetime(data['date']).dt.date)
                        if common_dates is None:
                            common_dates = dates
                        else:
                            common_dates = common_dates.intersection(dates)

                aggregation_results = {
                    'test_symbol': test_symbol,
                    'sources_available': len(source_data),
                    'total_records': sum(len(data) for data in source_data.values()),
                    'common_dates': len(common_dates) if common_dates else 0,
                    'recommended_source': best_source,
                    'source_priority': source_priority,
                    'completeness_scores': completeness_scores,
                    'coverage_scores': coverage_scores,
                    'aggregation_strategy': 'priority_weighted_selection'
                }

                self.logger.info(f"  å¯ç”¨æ•°æ®æº: {aggregation_results['sources_available']}")
                self.logger.info(f"  æ€»è®°å½•æ•°: {aggregation_results['total_records']}")
                self.logger.info(f"  å…±åŒæ—¥æœŸ: {aggregation_results['common_dates']}")
                self.logger.info(f"  æ¨èæ•°æ®æº: {aggregation_results['recommended_source']}")

            else:
                aggregation_results = {
                    'test_symbol': test_symbol,
                    'sources_available': len(source_data),
                    'status': 'insufficient_sources_for_aggregation'
                }
                self.logger.info("  æ•°æ®æºä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œèšåˆæ¼”ç¤º")

        except Exception as e:
            self.logger.error(f"æ•°æ®èšåˆæ¼”ç¤ºå¤±è´¥: {e}")
            aggregation_results = {'error': str(e)}

        self.aggregated_data = aggregation_results
        self.logger.info("âœ… æ•°æ®èšåˆæ¼”ç¤ºå®Œæˆ")
        return aggregation_results

    def run_benchmark_suite(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æºåŸºå‡†æµ‹è¯•å¥—ä»¶"""
        self.logger.info("ğŸ è¿è¡Œæ•°æ®æºåŸºå‡†æµ‹è¯•å¥—ä»¶...")

        benchmark_results = {
            'test_timestamp': datetime.now().isoformat(),
            'test_duration': 0,
            'sources_tested': len(self.data_sources),
            'symbols_tested': len(self.symbols),
            'test_period_days': self.test_period_days,
            'overall_ranking': {},
            'category_winners': {}
        }

        start_time = time.time()

        try:
            # ç»¼åˆè¯„åˆ†è®¡ç®—
            overall_scores = {}

            for source_name in self.data_sources.keys():
                perf_metrics = self.performance_metrics.get(source_name, {})
                quality_metrics = self.quality_reports.get(source_name, {})

                # è®¡ç®—ç»¼åˆè¯„åˆ† (0-100)
                performance_score = (
                    perf_metrics.get('reliability_score', 0) * 0.4 +  # å¯é æ€§40%
                    (100 - min(100, perf_metrics.get('avg_fetch_time', 10) * 10)) * 0.3 +  # é€Ÿåº¦30%
                    (perf_metrics.get('success_rate', 0) * 100) * 0.3  # æˆåŠŸç‡30%
                )

                quality_score = quality_metrics.get('quality_score', 0)

                # ç»¼åˆè¯„åˆ†ï¼šæ€§èƒ½60% + è´¨é‡40%
                overall_score = performance_score * 0.6 + quality_score * 0.4
                overall_scores[source_name] = overall_score

            # æ’åº
            ranked_sources = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
            benchmark_results['overall_ranking'] = {
                rank + 1: {'source': source, 'score': score}
                for rank, (source, score) in enumerate(ranked_sources)
            }

            # åˆ†ç±»è·èƒœè€…
            if self.performance_metrics:
                # é€Ÿåº¦å† å†›
                speed_winner = min(self.performance_metrics.items(),
                                 key=lambda x: x[1].get('avg_fetch_time', float('inf')))
                benchmark_results['category_winners']['speed'] = {
                    'source': speed_winner[0],
                    'avg_fetch_time': speed_winner[1].get('avg_fetch_time', 0)
                }

                # å¯é æ€§å† å†›
                reliability_winner = max(self.performance_metrics.items(),
                                       key=lambda x: x[1].get('reliability_score', 0))
                benchmark_results['category_winners']['reliability'] = {
                    'source': reliability_winner[0],
                    'reliability_score': reliability_winner[1].get('reliability_score', 0)
                }

            if self.quality_reports:
                # è´¨é‡å† å†›
                quality_winner = max(self.quality_reports.items(),
                                   key=lambda x: x[1].get('quality_score', 0))
                benchmark_results['category_winners']['quality'] = {
                    'source': quality_winner[0],
                    'quality_score': quality_winner[1].get('quality_score', 0)
                }

            benchmark_results['test_duration'] = time.time() - start_time

            # æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ
            self.logger.info("ğŸ† åŸºå‡†æµ‹è¯•ç»“æœ:")
            for rank, info in benchmark_results['overall_ranking'].items():
                self.logger.info(f"   ç¬¬{rank}å: {info['source']} (ç»¼åˆè¯„åˆ†: {info['score']:.1f})")

            if benchmark_results['category_winners']:
                self.logger.info("ğŸ¥‡ åˆ†ç±»å† å†›:")
                for category, winner in benchmark_results['category_winners'].items():
                    self.logger.info(f"   {category}: {winner['source']}")

        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            benchmark_results['error'] = str(e)

        self.benchmark_results = benchmark_results
        self.logger.info("âœ… åŸºå‡†æµ‹è¯•å¥—ä»¶å®Œæˆ")
        return benchmark_results

    def generate_ecosystem_report(self):
        """ç”Ÿæˆæ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤ºæŠ¥å‘Š"""
        self.logger.info("ğŸ“‹ ç”Ÿæˆæ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤ºæŠ¥å‘Š...")

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            'demo_type': 'DataSource Ecosystem Demo',
            'generation_time': datetime.now().isoformat(),
            'test_configuration': {
                'symbols_tested': self.symbols,
                'test_period_days': self.test_period_days,
                'sources_registered': len(self.data_sources)
            },
            'data_sources': {
                name: {
                    'config': self.source_configs.get(name, {}),
                    'performance': self.performance_metrics.get(name, {}),
                    'quality': self.quality_reports.get(name, {})
                }
                for name in self.data_sources.keys()
            },
            'aggregation_results': self.aggregated_data,
            'benchmark_results': self.benchmark_results,
            'ecosystem_features_demonstrated': [
                'å¤šæ•°æ®æºæ³¨å†Œå’Œç®¡ç†',
                'æ•°æ®æºæ€§èƒ½åŸºå‡†æµ‹è¯•',
                'æ•°æ®è´¨é‡åˆ†æå’Œè¯„ä¼°',
                'æ•°æ®èšåˆç­–ç•¥æ¼”ç¤º',
                'æ•°æ®æºä¼˜å…ˆçº§æ’åº',
                'ç¼“å­˜æœºåˆ¶å’Œä¼˜åŒ–',
                'é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶',
                'ç»¼åˆè¯„åˆ†å’Œæ’åç³»ç»Ÿ'
            ]
        }

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.output_dir / 'datasource_ecosystem_demo_report.json'
        with open(report_file, 'w') as f:
            json.dump(report, f, default=str, indent=2)

        # æ‰“å°æ‘˜è¦
        self.logger.info("ğŸ“Š æ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤ºç»“æœæ‘˜è¦:")
        self.logger.info(f"   æ³¨å†Œæ•°æ®æº: {len(self.data_sources)} ä¸ª")
        self.logger.info(f"   æµ‹è¯•æ ‡çš„: {len(self.symbols)} ä¸ª")
        self.logger.info(f"   æµ‹è¯•å‘¨æœŸ: {self.test_period_days} å¤©")

        if self.benchmark_results.get('overall_ranking'):
            winner = self.benchmark_results['overall_ranking'][1]
            self.logger.info(f"   ç»¼åˆå† å†›: {winner['source']} (è¯„åˆ†: {winner['score']:.1f})")

        if self.aggregated_data.get('recommended_source'):
            self.logger.info(f"   æ¨èæ•°æ®æº: {self.aggregated_data['recommended_source']}")

        self.logger.info(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        return report

    def run_demo(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤º"""
        self.logger.info("ğŸš€ å¼€å§‹æ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤º...")

        try:
            # 1. æ£€æŸ¥æ•°æ®æºå¯ç”¨æ€§
            mode = self.check_datasource_availability()

            # 2. è®¾ç½®æ•°æ®æº
            if not self.setup_data_sources():
                return None

            # 3. ç”Ÿæˆç¤ºä¾‹æ•°æ®
            if not self.generate_sample_data_files():
                return None

            # 4. æ€§èƒ½åŸºå‡†æµ‹è¯•
            performance_results = self.test_data_source_performance()

            # 5. æ•°æ®è´¨é‡åˆ†æ
            quality_results = self.perform_data_quality_analysis()

            # 6. æ•°æ®èšåˆæ¼”ç¤º
            aggregation_results = self.demonstrate_data_aggregation()

            # 7. è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶
            benchmark_results = self.run_benchmark_suite()

            # 8. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            final_report = self.generate_ecosystem_report()

            self.logger.info("ğŸ‰ æ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤ºå®Œæˆ!")
            return final_report

        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æºç”Ÿæ€ç³»ç»Ÿæ¼”ç¤ºå¤±è´¥: {e}")
            return None


if __name__ == '__main__':
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # è¿è¡Œæ¼”ç¤º
    demo = DataSourceEcosystemDemo()
    results = demo.run_demo()
