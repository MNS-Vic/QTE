# QTE Fluentd日志聚合配置
# 收集、处理和转发所有系统日志

# 输入源配置
<source>
  @type tail
  @id qte_application_logs
  path /app/logs/qte.log
  pos_file /var/log/fluentd-qte.log.pos
  tag qte.application
  format json
  time_key timestamp
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  keep_time_key true
  read_from_head true
  refresh_interval 1
</source>

<source>
  @type tail
  @id qte_error_logs
  path /app/logs/error.log
  pos_file /var/log/fluentd-error.log.pos
  tag qte.error
  format multiline
  format_firstline /^\d{4}-\d{2}-\d{2}/
  format1 /^(?<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) (?<level>\w+) (?<logger>[\w\.]+) - (?<message>.*)/
  time_key timestamp
  time_format %Y-%m-%d %H:%M:%S,%L
  keep_time_key true
  read_from_head true
</source>

<source>
  @type tail
  @id qte_trading_logs
  path /app/logs/trading.log
  pos_file /var/log/fluentd-trading.log.pos
  tag qte.trading
  format json
  time_key timestamp
  time_format %Y-%m-%dT%H:%M:%S.%LZ
  keep_time_key true
  read_from_head true
</source>

<source>
  @type tail
  @id nginx_access_logs
  path /var/log/nginx/access.log
  pos_file /var/log/fluentd-nginx-access.log.pos
  tag nginx.access
  format nginx
  time_key time_local
  keep_time_key true
  read_from_head true
</source>

<source>
  @type tail
  @id nginx_error_logs
  path /var/log/nginx/error.log
  pos_file /var/log/fluentd-nginx-error.log.pos
  tag nginx.error
  format /^(?<timestamp>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?<level>\w+)\] (?<pid>\d+)#(?<tid>\d+): (?<message>.*)/
  time_key timestamp
  time_format %Y/%m/%d %H:%M:%S
  keep_time_key true
  read_from_head true
</source>

# 系统指标收集
<source>
  @type prometheus
  @id prometheus_metrics
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_monitor
  @id prometheus_monitor
  interval 10
  <labels>
    host "#{Socket.gethostname}"
    service qte-fluentd
  </labels>
</source>

# 过滤器配置
<filter qte.**>
  @type record_transformer
  @id qte_enrichment
  <record>
    hostname "#{Socket.gethostname}"
    service qte
    environment production
    version "#{ENV['QTE_VERSION'] || '1.0.0'}"
  </record>
</filter>

<filter qte.application>
  @type parser
  @id qte_application_parser
  key_name message
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

<filter qte.error>
  @type record_transformer
  @id qte_error_enrichment
  <record>
    severity ERROR
    alert_required true
  </record>
</filter>

<filter qte.trading>
  @type record_transformer
  @id qte_trading_enrichment
  <record>
    category trading
    compliance_required true
  </record>
</filter>

# 错误检测和告警
<filter qte.**>
  @type grep
  @id error_detection
  <regexp>
    key level
    pattern ^(ERROR|CRITICAL|FATAL)$
  </regexp>
  <record>
    alert_level high
    notification_required true
  </record>
</filter>

# 性能监控
<filter qte.application>
  @type prometheus
  @id qte_log_metrics
  <metric>
    name qte_log_entries_total
    type counter
    desc Total number of log entries
    <labels>
      level ${level}
      service ${service}
    </labels>
  </metric>
  <metric>
    name qte_error_entries_total
    type counter
    desc Total number of error log entries
    <labels>
      level ${level}
      logger ${logger}
    </labels>
  </metric>
</filter>

# 输出配置
<match qte.error>
  @type copy
  <store>
    @type elasticsearch
    @id elasticsearch_error_output
    host elasticsearch
    port 9200
    index_name qte-errors
    type_name _doc
    logstash_format true
    logstash_prefix qte-errors
    logstash_dateformat %Y.%m.%d
    include_tag_key true
    tag_key @log_name
    flush_interval 1s
    <buffer>
      @type file
      path /var/log/fluentd-buffers/elasticsearch-error
      flush_mode interval
      retry_type exponential_backoff
      flush_thread_count 2
      flush_interval 5s
      retry_forever
      retry_max_interval 30
      chunk_limit_size 2M
      queue_limit_length 8
      overflow_action block
    </buffer>
  </store>
  <store>
    @type webhook
    @id webhook_alert_output
    endpoint "#{ENV['ALERT_WEBHOOK_URL']}"
    http_method post
    serializer json
    rate_limit_msec 1000
    <format>
      @type json
    </format>
    <buffer>
      flush_mode immediate
      retry_type exponential_backoff
      retry_forever false
      retry_max_times 3
    </buffer>
  </store>
</match>

<match qte.trading>
  @type elasticsearch
  @id elasticsearch_trading_output
  host elasticsearch
  port 9200
  index_name qte-trading
  type_name _doc
  logstash_format true
  logstash_prefix qte-trading
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  <buffer>
    @type file
    path /var/log/fluentd-buffers/elasticsearch-trading
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 10s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 5M
    queue_limit_length 16
    overflow_action block
  </buffer>
</match>

<match qte.application>
  @type elasticsearch
  @id elasticsearch_application_output
  host elasticsearch
  port 9200
  index_name qte-application
  type_name _doc
  logstash_format true
  logstash_prefix qte-application
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  <buffer>
    @type file
    path /var/log/fluentd-buffers/elasticsearch-application
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 30s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 10M
    queue_limit_length 32
    overflow_action block
  </buffer>
</match>

<match nginx.**>
  @type elasticsearch
  @id elasticsearch_nginx_output
  host elasticsearch
  port 9200
  index_name qte-nginx
  type_name _doc
  logstash_format true
  logstash_prefix qte-nginx
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  <buffer>
    @type file
    path /var/log/fluentd-buffers/elasticsearch-nginx
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 1
    flush_interval 60s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 8M
    queue_limit_length 16
    overflow_action block
  </buffer>
</match>

# 备份输出到文件
<match **>
  @type file
  @id file_backup_output
  path /var/log/fluentd-backup/qte.%Y%m%d
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  time_format %Y%m%dT%H%M%S%z
  compress gzip
  <buffer time>
    timekey 1d
    timekey_wait 10m
    timekey_use_utc true
    path /var/log/fluentd-buffers/file-backup
  </buffer>
  <format>
    @type json
  </format>
</match>
